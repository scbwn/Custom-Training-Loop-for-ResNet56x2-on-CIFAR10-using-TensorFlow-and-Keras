import os
import numpy as np
import tensorflow as tf
import random as rn
np.random.seed(1)
rn.seed(2)
tf.random.set_seed(3)

from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.utils import resample
from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score
from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical, Progbar
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Layer, Input, Dense, Dropout, BatchNormalization, Activation, Add, Multiply, Lambda
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, UpSampling2D, GlobalAveragePooling2D
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Normalization, Resizing, RandomCrop, RandomFlip
from tensorflow.keras.activations import softmax
from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam
from copy import deepcopy
import time


n_epoch=150
batch_size=100

d=(32,32,3)
num_of_classes=10

## Verbose
train_verbose = 1

# Function definition
def set_seed_TF2(seed):
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    rn.seed(seed)
    
def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    # SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]


def crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)
    
def resnet_layer(x,
                 num_filters,
                 kernel_size=3,
                 strides=1,
                 activation='relu',
                 batch_normalization=True):

    conv = Conv2D(num_filters,
                  kernel_size=kernel_size,
                  strides=strides,
                  padding='same',
                  kernel_initializer='he_normal',
                  kernel_regularizer=regularizers.l2(5E-4))
    x = conv(x)
    if batch_normalization:
        x = BatchNormalization()(x)
    if activation is not None:
        x = Activation(activation)(x)
    return x

def building_blocks(x, filters=16*2, depth=56):
    if (depth - 2) % 6 != 0:
        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')
    num_filters = filters
    num_res_blocks = int((depth - 2) / 6)

    x = resnet_layer(x, num_filters=num_filters)
    for stack in range(3):
        for res_block in range(num_res_blocks):
            strides = 1
            if stack > 0 and res_block == 0:
                strides = 2  # downsample
            y = resnet_layer(x,
                             num_filters=num_filters,
                             strides=strides)
            y = resnet_layer(y,
                             num_filters=num_filters,
                             activation=None)
            if stack > 0 and res_block == 0:  # first layer but not first stack
                pad_dim=num_filters-x.shape[-1]
                paddings=tf.constant([[0, 0,], [0, 0], [0, 0], [pad_dim-pad_dim//2, pad_dim//2]])
                x = tf.pad(x[:, ::2, ::2, :], paddings, mode="CONSTANT")
            x = Add()([x, y])
            x = Activation('relu')(x)
        num_filters*=2
    return x

def create_model(d, num_of_classes):
    set_seed_TF2(100)
    inp=Input(shape=d)
    x = building_blocks(inp)
    x = GlobalAveragePooling2D()(x)
    op=Dense(num_of_classes, activation='softmax')(x)
    nn = Model(inputs=inp, outputs=op)
    return nn

def lr_scheduler(step):
    lr=0.01
    if step>=5000:
        lr=0.05
    if step>=60000:
        lr=0.005
    if step>=70000:
        lr=0.0005
    return lr

def confidence_interval(a,l):
    import numpy as np, scipy.stats as st
    return st.t.interval(l, len(a)-1, loc=np.mean(a), scale=st.sem(a))


# Dataset Preprocessing
from tensorflow.keras.datasets import cifar10
(x_train, y_train), (x_test, y_test)=cifar10.load_data()

x_train=x_train.astype(np.float32)/255
x_test=x_test.astype(np.float32)/255

# Scaling
mean=x_train.mean((0,1,2))
std=x_train.std((0,1,2))
paddings = tf.constant([[0, 0,], [4, 4], [4, 4], [0, 0]])
x_train = tf.pad(x_train, paddings, mode="CONSTANT")
x_train=(x_train-mean)/std
x_test=(x_test-mean)/std

y_train=y_train.reshape(-1,1)
y_test=y_test.reshape(-1,1)


# One-hot Encoding
y_train=to_categorical(y_train)


# Data generator for training data
from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_generator = ImageDataGenerator(horizontal_flip = True)

# Generate training batches
train_batches = train_generator.flow(x_train, y_train, batch_size=batch_size)
train_batches = crop_generator(train_batches, d[0])

callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_scheduler)]


model=create_model(d,num_of_classes)

# Instantiate an optimizer to train the model.
optimizer = SGD(momentum=0.9, nesterov=True)
# Instantiate a loss function.
loss_fn = tf.keras.losses.CategoricalCrossentropy()

# Prepare the metrics.
train_acc_metric = tf.keras.metrics.CategoricalAccuracy()

metrics_names = ['loss','acc']

@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        y_hat = model(x, training=True)
        loss_value = loss_fn(y, y_hat)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, y_hat)
    return loss_value


for epoch in range(n_epoch):
    print("\nEpoch {}/{}".format(epoch+1,n_epoch))
    pb = Progbar(x_train.shape[0], stateful_metrics=metrics_names)

    # Iterate over the batches of the dataset.
    for step in range(x_train.shape[0]//batch_size):
        x_batch_train, y_batch_train=next(train_batches)
        optimizer.learning_rate = lr_scheduler((step+1)*(epoch+1))
        loss_value = train_step(x_batch_train, y_batch_train)
        train_acc = train_acc_metric.result()
        
        values=[('loss',loss_value), ('acc',train_acc)]
        
        pb.add(batch_size, values=values)
    

    # Reset training metrics at the end of each epoch
    train_acc_metric.reset_state()


model.save('./models/cifar10_resnet56x2.h5')

y_pred_test=model.predict(x_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print('Test Classification')
print(classification_report(y_test, np.argmax(y_pred_test,1)))
print(confusion_matrix(y_test, np.argmax(y_pred_test,1)))
